{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network(Pytorch) \n",
    "# Classify fashion items in the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "You will build a four-layer fully-connected neural network through the well-known deep learning framework pytorch, analyze 60,000 training set images and 10,000 test set images in the Fashion-MNIST dataset, and observe the training error and verification error with the training algebra. \n",
    "\n",
    "Firstly, an ordinary four-layer fully connected neural network was built. After training, it was found that the fitting was obvious. Reduce the overfitting by the Dropout method. You will master image classification, accuracy and error analysis, build a fully connected neural network and perform gradient descent training through the Adam algorithm.\n",
    "\n",
    "\n",
    "![Fashion-MNIST Dataset ](https://upload-images.jianshu.io/upload_images/13714448-e0d2b1be2bb1702e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Fasion-MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covers positive images of 70,000 different products from 10 classification. \n",
    "The size, format, and training set/test set partitioning of Fashion-MNIST is identical to the original MNIST. \n",
    "\n",
    "60000/10000 training test data division, 28x28 grayscale picture. \n",
    "You can use it directly to test the performance of your machine learning and deep learning algorithms without changing any code.\n",
    "\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist/blob/master/README.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "from torch import nn, optim  \n",
    "import torch.nn.functional as F \n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: normalize image data so that grayscale data is between -1 and +1\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download the Fashion-MNIST training dataset and build the training set data loader trainloader, each time loading 64 images from the training set.\n",
    "trainset = datasets.FashionMNIST('dataset/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download the Fashion-MNIST testing dataset and build the training set data loader trainloader, each time loading 64 images from the training set.\n",
    "testset = datasets.FashionMNIST('dataset/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding label for this image is Trouser\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "\n",
    "#There are 64 images, we look at the image with index 3.\n",
    "imagedemo = image[3]\n",
    "imagedemolabel = label[3]\n",
    "\n",
    "imagedemo = imagedemo.reshape((28,28))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(imagedemo)\n",
    "\n",
    "labellist = ['T-shirt','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "print(f'The corresponding label for this image is {labellist[imagedemolabel]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 6, 2, 5, 3, 2, 2, 1, 3, 9, 6, 4, 2, 7, 4, 8, 9, 1, 2, 7, 7, 1,\n",
       "        1, 1, 2, 4, 1, 9, 0, 4, 3, 8, 7, 2, 0, 6, 1, 3, 0, 0, 5, 8, 0, 4, 1, 3,\n",
       "        8, 9, 1, 4, 8, 5, 0, 8, 6, 3, 7, 7, 8, 6, 6, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Label\" contains the label corresponding to the 64 images in the \"image\"\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top|\n",
    "| 1 | Trouser|\n",
    "| 2 | Pullover|\n",
    "| 3 | Dress|\n",
    "| 4 | Coat|\n",
    "| 5 | Sandal|\n",
    "| 6 | Shirt|\n",
    "| 7 | Sneaker|\n",
    "| 8 | Bag|\n",
    "| 9 | Ankle boot|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a image in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedemo = image[2]\n",
    "imagedemolabel = label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagedemo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedemo = imagedemo.reshape((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagedemo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagedemolabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding label for this image is T-shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(imagedemo)\n",
    "\n",
    "labellist = ['T-shirt','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "print(f'The corresponding label for this image is {labellist[imagedemolabel]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train a four-layer fully connected neural network\n",
    "\n",
    "The input to the neural network is 28 * 28 = 784 pixels<br>\n",
    "The first hidden layer contains 256 neurons<br>\n",
    "The second hidden layer contains 128 neurons<br>\n",
    "The third hidden layer contains 64 neurons<br>\n",
    "The output layer outputs 10 results, corresponding to 10 classification of images.<br>\n",
    "\n",
    "![全连接神经网络](https://upload-images.jianshu.io/upload_images/13714448-c8b5fa11504798bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Learning times: 1/15..  training loss: 0.517..  Validation loss: 0.447..  accuracy: 0.837\n",
      "Learning times: 2/15..  training loss: 0.391..  Validation loss: 0.423..  accuracy: 0.844\n",
      "Learning times: 3/15..  training loss: 0.354..  Validation loss: 0.391..  accuracy: 0.861\n",
      "Learning times: 4/15..  training loss: 0.331..  Validation loss: 0.370..  accuracy: 0.864\n",
      "Learning times: 5/15..  training loss: 0.316..  Validation loss: 0.407..  accuracy: 0.859\n",
      "Learning times: 6/15..  training loss: 0.301..  Validation loss: 0.366..  accuracy: 0.870\n",
      "Learning times: 7/15..  training loss: 0.289..  Validation loss: 0.369..  accuracy: 0.868\n",
      "Learning times: 8/15..  training loss: 0.282..  Validation loss: 0.384..  accuracy: 0.868\n",
      "Learning times: 9/15..  training loss: 0.271..  Validation loss: 0.379..  accuracy: 0.868\n",
      "Learning times: 10/15..  training loss: 0.265..  Validation loss: 0.386..  accuracy: 0.875\n",
      "Learning times: 11/15..  training loss: 0.257..  Validation loss: 0.360..  accuracy: 0.880\n",
      "Learning times: 12/15..  training loss: 0.250..  Validation loss: 0.380..  accuracy: 0.877\n",
      "Learning times: 13/15..  training loss: 0.248..  Validation loss: 0.361..  accuracy: 0.880\n",
      "Learning times: 14/15..  training loss: 0.242..  Validation loss: 0.367..  accuracy: 0.879\n",
      "Learning times: 15/15..  training loss: 0.234..  Validation loss: 0.368..  accuracy: 0.880\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# The optimization method is the Adam gradient descent method with a learning rate of 0.003.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Learn all the data in the training dataset 15 times. The larger the number, the longer the training time.\n",
    "epochs = 15\n",
    "\n",
    "# Store training and test errors for each training in both lists.\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "print('Training')\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Do the following operations each time you finish the dataset\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        # No need to open automatic derivation and back propagation when testing\n",
    "        with torch.no_grad():\n",
    "            # Close Dropout\n",
    "            model.eval()\n",
    "            \n",
    "            for images, labels in testloader:\n",
    "    \n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                \n",
    "                # The right side of the equal sign predicts the correct proportion of each batch of 64 test images.\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        # Open Dropout\n",
    "        model.train()\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Learning times: {}/{}.. \".format(e+1, epochs),\n",
    "              \"training loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Validation loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7dd3861320>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that although the training loss has been declining, the validation loss is still high.<br> \n",
    "(Our neural network is like a high-scoring and low-energy classmate.<br>\n",
    "Usually, all the answers to the after-school questions are memorized.)<br>\n",
    "<br>\n",
    "Sometimes the predicted probability is only 30 to 40 percent of the grasp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "img = img.reshape((28,28)).numpy()\n",
    "plt.imshow(img)\n",
    "\n",
    "# Convert test images to one-dimensional column vectors\n",
    "img = torch.from_numpy(img)\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Perform forward inference to predict the classification of the image\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "ps = torch.exp(output)\n",
    "\n",
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "labellist = ['T-shirt','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "prediction = labellist[top_class]\n",
    "probability = float(top_p)\n",
    "print(f'The neural network guesses the image is {prediction}，probability is {probability*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Dropout method to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Large-scale neural networks have two disadvantages：<br>\n",
    "1.Spend long time.<br>\n",
    "2.Overfitting<br>\n",
    "<br>\n",
    "The Dropout is a good solution to this problem. Every time you do a dropout, it is equivalent to finding a thinner network from the original network.\n",
    "\n",
    "![Dropout](https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blogs/imgs/n7-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Randomly \"die\" 20% of neurons during each training session to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Make sure that the input tensor is an expanded single-column data, compressing the three dimensions of the channel, length, and width of each image into one column.\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        \n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# The optimization method is the Adam gradient descent method with a learning rate of 0.003.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Learn all the data in the training dataset 15 times. The larger the number, the longer the training time.\n",
    "epochs = 15\n",
    "\n",
    "# Store training and test errors for each training in both lists.\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "print('Training')\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Do the following operations each time you finish the dataset\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        # No need to open automatic derivation and back propagation when testing\n",
    "        with torch.no_grad():\n",
    "            # Close Dropout\n",
    "            model.eval()\n",
    "            \n",
    "            for images, labels in testloader:\n",
    "    \n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                \n",
    "                # The right side of the equal sign predicts the correct proportion of each batch of 64 test images.\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        # Open Dropout\n",
    "        model.train()\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Learning times: {}/{}.. \".format(e+1, epochs),\n",
    "              \"training loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Validation loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the Training loss and the Validation loss are gradually reduced as the number of learning increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "img = img.reshape((28,28)).numpy()\n",
    "plt.imshow(img)\n",
    "\n",
    "# Convert test images to one-dimensional column vectors\n",
    "img = torch.from_numpy(img)\n",
    "img = img.view(1, 784)\n",
    "\n",
    "# Perform forward inference to predict the classification of the image\n",
    "with torch.no_grad():\n",
    "    output = model.forward(img)\n",
    "ps = torch.exp(output)\n",
    "\n",
    "top_p, top_class = ps.topk(1, dim=1)\n",
    "labellist = ['T-shirt','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "prediction = labellist[top_class]\n",
    "probability = float(top_p)\n",
    "print(f'The neural network guesses the image is {prediction}，probability is {probability*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
