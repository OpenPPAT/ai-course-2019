{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce to OpenAI Gym with DuckieNav Environment\n",
    "\n",
    "\n",
    "We will introduce the main API methods in gym:\n",
    "* `reset()`\n",
    "* `step()`\n",
    "* `render()`\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/rl-diagram.png\"  width=\"480\" height=\"480\">\n",
    "\n",
    "and the essentials in RL:\n",
    "* Environment\n",
    "* State\n",
    "* Action\n",
    "* Reward\n",
    "* Agent\n",
    "\n",
    "\n",
    "\n",
    "### Environment\n",
    "The example is modifed from the Taxi Problem in \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by  Tom Dietterich (2000), Journal of Artificial Intelligence Research.\n",
    "\n",
    "<!--img style=\"float: right;\" src=\"images/DuckieNav-v1.png\"  width=\"240\" height=\"240\"-->\n",
    "\n",
    "\n",
    "```\n",
    "MAP = [\n",
    "    \"+-----------------+\",\n",
    "    \"|O|O| : : : : :G: |\",\n",
    "    \"|O|O| |O| |O| |O| |\",\n",
    "    \"|O| : |O| |O| |O| |\",\n",
    "    \"| : |O|O| : : : : |\",\n",
    "    \"| |O|O|O|O|O| |O| |\",\n",
    "    \"| : :R: : : : |O| |\",\n",
    "    \"| |O|O|O| |O|O|O| |\",\n",
    "    \"| |O| : : |O| : : |\",\n",
    "    \"| |O| |O|O|O|B|O| |\",\n",
    "    \"| : : : : : : |O| |\",\n",
    "    \"| |O| |O| |O| |O| |\",\n",
    "    \"| : : : : : : |O| |\",\n",
    "    \"| |O| |O| |O|O|O| |\",\n",
    "    \"| : : :Y: : : : : |\",\n",
    "    \"+-----------------+\",\n",
    "]\n",
    "```\n",
    "\n",
    "We consider shows a 14 by 9 grid world, except the \"service area.\" The taxi problem is episodic, and in each episode a passenger is located at one of the 4 specially designated locations (R, Y, B, and G). The taxi(agent) starts in a given location and must go to the transported passengerâ€™s location, pick up the passenger, go to the destination location, and put down the passenger. The episode ends when the passenger is deposited at the destination location to one of the 4 locations.\n",
    "\n",
    "\n",
    "Adapted from https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize DuckieNav-v0\n",
    "\n",
    "### Installation\n",
    "You can obtain and install this customized gym environment (https://github.com/ARG-NCTU/gym-duckienav.git): \n",
    "```\n",
    "$ cd ~/gym-duckienav\n",
    "$ git pull\n",
    "$ pip install -e . # you may need sudo depending on your setup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_duckienav\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"DuckieNav-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many 'states' in observation_space: \n",
    "There are 2520 states from: 14 (rows) x 9 (columns) x 5 (passenger locations: R, Y, B, G, or on taxi) x 4 (destinations: R, Y, B, or G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2520"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action_space: \n",
    "There are 6 possible actions in Taxi-v2 environment\n",
    "* down (0), up (1), right (2), left (3), pick-up (4), and drop-off (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. States\n",
    "\n",
    "Resets the state of the environment and returns an initial observation (state).\n",
    "\n",
    "The current state is from :\n",
    "* current taxi row position\n",
    "* current taxi colum position\n",
    "* passenger location (Blue or in taxi) from 0: R, 1: G, 2: Y, 3: B; 4: in taxi.\n",
    "* destination location (Magenta) from 0: R, 1: G, 2: Y, 3: B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 2206\n",
      "12\n",
      "2\n",
      "1\n",
      "2\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O|\u001b[43m \u001b[0m|O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "print \"Current state: \" + str(env.s)\n",
    "for p in env.decode(env.s): print p\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat previous cell for a few times.\n",
    "\n",
    "In taxi problem, the colors mean:\n",
    "* blue: passenger's current position\n",
    "* magenta: destination\n",
    "* yellow: empty taxi\n",
    "* green: full taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Actions\n",
    "\n",
    "Remember that the taxi agent can perform the following actions:\n",
    "* 0: \"South\", \n",
    "* 1: \"North\", \n",
    "* 2: \"East\", \n",
    "* 3: \"West\", \n",
    "* 4: \"Pickup\", \n",
    "* 5: \"Dropoff\"\n",
    "\n",
    "Let's set the state to 124.\n",
    "Let the taxi agent perform some actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.s = 124\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `step()`\n",
    "\n",
    "Run one timestep of the environment's dynamics. \n",
    "It returns a tuple (observation, reward, done, info)\n",
    "* observation (object): agent's observation of the current environment\n",
    "* reward (float) : amount of reward returned after previous action\n",
    "* done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "* info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "\n",
    "Essentially the empty taxi is supposed to: \n",
    "* move toward the blue letter, \n",
    "* pickup the passenger (now the taxi is green), \n",
    "* drive to the magenta letter, and \n",
    "* drop the passenger (the taxi is yellow again).\n",
    "\n",
    "It is obvious that we should start with moving \"East\" env.step(2). Index 2 is for moving \"East\"\n",
    "We will do the followings:\n",
    "* Perform \"Pickup\" step(4) (although the passenger is not here)\n",
    "* Perform \"East\" step(2)\n",
    "* Perform \"Pickup\" step(4)\n",
    "* Perform \"West\" step(3)\n",
    "* Perform \"South\" step(0) for 5 times\n",
    "* Perfomr \"Dropoff\" (5)\n",
    "* Perform \"West\" step(3) for 4 times\n",
    "* Perfomr \"Dropoff\" (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "reward: -10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (East)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(2)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[42mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[42m_\u001b[0m:G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(3)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    env.step(0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n",
      "reward: -10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(5)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n",
      "reward: 20\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(5)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)\n",
    "print \"done: \" + str(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "You have probably figured out the rewards:\n",
    "* Perform any movements: -1\n",
    "* Pick up or drop off at the wrong position: -10\n",
    "* Drop off the passenger at the right position: 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ramdon Agent: \n",
    "\n",
    "We will use the funciton env.action_space.sample(); you could run the following cell for a few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good does behaving completely random do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 4021 Steps with a total reward of -15970\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "counter = 0\n",
    "g = 0\n",
    "reward = None\n",
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "    g += reward\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter,g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may luck out and solve the environment fairly quickly, but on average, a completely random policy will solve this environment in about ???? steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent with Basic Reinforcement Learning: Q-Learning\n",
    "\n",
    "In order to maximize our reward, we will have to have the algorithm remember its actions and their associated rewards. Here, the algorithmâ€™s memory is going to be a Q action value table.\n",
    "\n",
    "To manage this Q table, we will use a NumPy array. The size of this table will be the number of states (2520) by the number of possible actions (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| :\u001b[43m \u001b[0m:\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "\n",
      "Initial State = 924\n",
      "Step: 0, Action: 0, Reward: -1, Q[924] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 1, Action: 0, Reward: -1, Q[1104] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 2, Action: 0, Reward: -1, Q[1284] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 3, Action: 0, Reward: -1, Q[1464] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 4, Action: 0, Reward: -1, Q[1644] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 5, Action: 0, Reward: -1, Q[1824] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 6, Action: 0, Reward: -1, Q[2004] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 7, Action: 0, Reward: -1, Q[2184] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 8, Action: 0, Reward: -1, Q[2364] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 9, Action: 1, Reward: -1, Q[2364] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 10, Action: 1, Reward: -1, Q[2184] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 11, Action: 1, Reward: -1, Q[2004] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 12, Action: 1, Reward: -1, Q[1824] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 13, Action: 1, Reward: -1, Q[1644] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 14, Action: 1, Reward: -1, Q[1464] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 15, Action: 1, Reward: -1, Q[1284] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 16, Action: 1, Reward: -1, Q[1104] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 17, Action: 1, Reward: -1, Q[924] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 18, Action: 0, Reward: -1, Q[744] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 19, Action: 2, Reward: -1, Q[924] \t[-0.618 -0.618 -0.618  0.     0.     0.   ]\n",
      "Final State = 956\n",
      "Solved in 5006 Steps with a total reward of -11087\n",
      "[-0.618 -0.618 -0.618 -0.618 -6.18  12.36 ]\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = np.zeros([n_states, n_actions])\n",
    "\n",
    "episodes = 1\n",
    "G = 0\n",
    "counter = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    firstState = state\n",
    "    print(\"Initial State = {}\".format(state))\n",
    "    while reward != 20:\n",
    "        action = np.argmax(Q[state])  #1\n",
    "        state2, reward, done, info = env.step(action) #2\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) #3\n",
    "        \n",
    "        if counter < 20:\n",
    "            print(\"Step: {}, Action: {}, Reward: {}, Q[{}] \\t{}\".format(counter, action, reward, state, Q[state]))\n",
    "\n",
    "        counter += 1\n",
    "        G += reward\n",
    "        state = state2\n",
    "        \n",
    "finalState = state\n",
    "print(\"Final State = {}\".format(finalState))\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter, G))\n",
    "\n",
    "print Q[finalState]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (#1): The agent starts by choosing an action with the highest Q value for the current state using argmax. Argmax will return the index/action with the highest value for that state. Initially, our Q table will be all zeros. But, after every step, the Q values for state-action pairs will be updated.\n",
    "\n",
    "Second (#2): The agent then takes action and we store the future state as state2 (S t+1). This will allow the agent to compare the previous state to the new state.\n",
    "\n",
    "Third (#3): We update the state-action pair (St , At) for Q using the reward, and the max Q value for state2 (S t+1). This update is done using the action value formula (based upon the Bellman equation) and allows state-action pairs to be updated in a recursive fashion (based on future values). See the following Figure for the value iteration update.\n",
    "\n",
    "<img src=\"images/qlearn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run over multiple episodes so that we can converge on a optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Total Reward: -596\n",
      "Episode 200 Total Reward: -315\n",
      "Episode 300 Total Reward: -193\n",
      "Episode 400 Total Reward: 0\n",
      "Episode 500 Total Reward: -144\n",
      "Episode 600 Total Reward: -40\n",
      "Episode 700 Total Reward: -67\n",
      "Episode 800 Total Reward: -31\n",
      "Episode 900 Total Reward: -3\n",
      "Episode 1000 Total Reward: -21\n",
      "Episode 1100 Total Reward: -612\n",
      "Episode 1200 Total Reward: -114\n",
      "Episode 1300 Total Reward: -72\n",
      "Episode 1400 Total Reward: 5\n",
      "Episode 1500 Total Reward: 2\n",
      "Episode 1600 Total Reward: 4\n",
      "Episode 1700 Total Reward: 6\n",
      "Episode 1800 Total Reward: 2\n",
      "Episode 1900 Total Reward: 9\n",
      "Episode 2000 Total Reward: 4\n",
      "Episode 2100 Total Reward: 2\n",
      "Episode 2200 Total Reward: -1\n",
      "Episode 2300 Total Reward: 4\n",
      "Episode 2400 Total Reward: 3\n",
      "Episode 2500 Total Reward: 2\n",
      "Episode 2600 Total Reward: 4\n",
      "Episode 2700 Total Reward: 4\n",
      "Episode 2800 Total Reward: -10\n",
      "Episode 2900 Total Reward: 2\n",
      "Episode 3000 Total Reward: 1\n",
      "Episode 3100 Total Reward: -2\n",
      "Episode 3200 Total Reward: 7\n",
      "Episode 3300 Total Reward: 0\n",
      "Episode 3400 Total Reward: 5\n",
      "Episode 3500 Total Reward: -4\n",
      "Episode 3600 Total Reward: -14\n",
      "Episode 3700 Total Reward: -6\n",
      "Episode 3800 Total Reward: 2\n",
      "Episode 3900 Total Reward: 3\n",
      "Episode 4000 Total Reward: 11\n",
      "Episode 4100 Total Reward: 0\n",
      "Episode 4200 Total Reward: 4\n",
      "Episode 4300 Total Reward: 0\n",
      "Episode 4400 Total Reward: 2\n",
      "Episode 4500 Total Reward: 1\n",
      "Episode 4600 Total Reward: 2\n",
      "Episode 4700 Total Reward: 6\n",
      "Episode 4800 Total Reward: 0\n",
      "Episode 4900 Total Reward: 2\n",
      "Episode 5000 Total Reward: -1\n",
      "Episode 5100 Total Reward: 10\n",
      "Episode 5200 Total Reward: 4\n",
      "Episode 5300 Total Reward: 8\n",
      "Episode 5400 Total Reward: 8\n",
      "Episode 5500 Total Reward: -16\n",
      "Episode 5600 Total Reward: 8\n",
      "Episode 5700 Total Reward: -2\n",
      "Episode 5800 Total Reward: 7\n",
      "Episode 5900 Total Reward: -7\n",
      "Episode 6000 Total Reward: -6\n",
      "Episode 6100 Total Reward: 4\n",
      "Episode 6200 Total Reward: -5\n",
      "Episode 6300 Total Reward: -4\n",
      "Episode 6400 Total Reward: 9\n",
      "Episode 6500 Total Reward: -11\n",
      "Episode 6600 Total Reward: 3\n",
      "Episode 6700 Total Reward: -5\n",
      "Episode 6800 Total Reward: 3\n",
      "Episode 6900 Total Reward: -10\n",
      "Episode 7000 Total Reward: -4\n",
      "Episode 7100 Total Reward: -6\n",
      "Episode 7200 Total Reward: 4\n",
      "Episode 7300 Total Reward: 5\n",
      "Episode 7400 Total Reward: -3\n",
      "Episode 7500 Total Reward: -11\n",
      "Episode 7600 Total Reward: 3\n",
      "Episode 7700 Total Reward: 9\n",
      "Episode 7800 Total Reward: 2\n",
      "Episode 7900 Total Reward: 8\n",
      "Episode 8000 Total Reward: -2\n",
      "Episode 8100 Total Reward: 4\n",
      "Episode 8200 Total Reward: -2\n",
      "Episode 8300 Total Reward: -6\n",
      "Episode 8400 Total Reward: 8\n",
      "Episode 8500 Total Reward: 3\n",
      "Episode 8600 Total Reward: 0\n",
      "Episode 8700 Total Reward: -9\n",
      "Episode 8800 Total Reward: 5\n",
      "Episode 8900 Total Reward: 1\n",
      "Episode 9000 Total Reward: 6\n",
      "Episode 9100 Total Reward: 9\n",
      "Episode 9200 Total Reward: -9\n",
      "Episode 9300 Total Reward: 7\n",
      "Episode 9400 Total Reward: -6\n",
      "Episode 9500 Total Reward: 1\n",
      "Episode 9600 Total Reward: -10\n",
      "Episode 9700 Total Reward: 10\n",
      "Episode 9800 Total Reward: 4\n",
      "Episode 9900 Total Reward: -1\n",
      "Episode 10000 Total Reward: 3\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "rewardTracker = []\n",
    "\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state]) \n",
    "        state2, reward, done, info = env.step(action) \n",
    "        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,G))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have learned the optimal Q Values we have developed a optimal policy and have no need to train the agent anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :\u001b[43mO\u001b[0m: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |\u001b[43mO\u001b[0m| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : :\u001b[43m \u001b[0m: |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |\u001b[43mO\u001b[0m| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |\u001b[43mO\u001b[0m| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[42mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |\u001b[42mO\u001b[0m| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |\u001b[42mO\u001b[0m| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : :\u001b[42m_\u001b[0m: |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : :\u001b[42m_\u001b[0m: : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O|\u001b[42m_\u001b[0m|O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : :\u001b[42m_\u001b[0m: :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: :\u001b[42m_\u001b[0m: : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R:\u001b[42m_\u001b[0m: : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|\u001b[42mO\u001b[0m| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| :\u001b[42m_\u001b[0m: |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |\u001b[42mO\u001b[0m|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : :\u001b[42m_\u001b[0m: : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |\u001b[42mO\u001b[0m| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : :\u001b[42m_\u001b[0m: : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |\u001b[42mO\u001b[0m| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35m\u001b[42mY\u001b[0m\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35m\u001b[42mY\u001b[0m\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "done = None\n",
    "\n",
    "while done != True:\n",
    "    # We simply take the action with the highest Q Value\n",
    "    action = np.argmax(Q[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
