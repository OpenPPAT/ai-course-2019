{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce to OpenAI Gym with DuckieNav Environment\n",
    "\n",
    "\n",
    "We will introduce the main API methods in gym:\n",
    "* `reset()`\n",
    "* `step()`\n",
    "* `render()`\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/rl-diagram.png\"  width=\"480\" height=\"480\">\n",
    "\n",
    "and the essentials in RL:\n",
    "* Environment\n",
    "* State\n",
    "* Action\n",
    "* Reward\n",
    "* Agent\n",
    "\n",
    "\n",
    "\n",
    "### Environment\n",
    "The example is modifed from the Taxi Problem in \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by  Tom Dietterich (2000), Journal of Artificial Intelligence Research.\n",
    "\n",
    "<!--img style=\"float: right;\" src=\"images/DuckieNav-v1.png\"  width=\"240\" height=\"240\"-->\n",
    "\n",
    "\n",
    "```\n",
    "MAP = [\n",
    "    \"+-----------------+\",\n",
    "    \"|O|O| : : : : :G: |\",\n",
    "    \"|O|O| |O| |O| |O| |\",\n",
    "    \"|O| : |O| |O| |O| |\",\n",
    "    \"| : |O|O| : : : : |\",\n",
    "    \"| |O|O|O|O|O| |O| |\",\n",
    "    \"| : :R: : : : |O| |\",\n",
    "    \"| |O|O|O| |O|O|O| |\",\n",
    "    \"| |O| : : |O| : : |\",\n",
    "    \"| |O| |O|O|O|B|O| |\",\n",
    "    \"| : : : : : : |O| |\",\n",
    "    \"| |O| |O| |O| |O| |\",\n",
    "    \"| : : : : : : |O| |\",\n",
    "    \"| |O| |O| |O|O|O| |\",\n",
    "    \"| : : :Y: : : : : |\",\n",
    "    \"+-----------------+\",\n",
    "]\n",
    "```\n",
    "\n",
    "We consider shows a 14 by 9 grid world, except the \"service area.\" The taxi problem is episodic, and in each episode a passenger is located at one of the 4 specially designated locations (R, Y, B, and G). The taxi(agent) starts in a given location and must go to the transported passenger’s location, pick up the passenger, go to the destination location, and put down the passenger. The episode ends when the passenger is deposited at the destination location to one of the 4 locations.\n",
    "\n",
    "\n",
    "Adapted from https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize DuckieNav-v0\n",
    "\n",
    "### Installation\n",
    "You can obtain and install this customized gym environment (https://github.com/ARG-NCTU/gym-duckienav.git): \n",
    "```\n",
    "$ cd ~/gym-duckienav\n",
    "$ git pull\n",
    "$ pip install -e . # you may need sudo depending on your setup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_duckienav\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"DuckieNav-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many 'states' in observation_space: \n",
    "There are 2520 states from: 14 (rows) x 9 (columns) x 5 (passenger locations: R, Y, B, G, or on taxi) x 4 (destinations: R, Y, B, or G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2520"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action_space: \n",
    "There are 6 possible actions in Taxi-v2 environment\n",
    "* down (0), up (1), right (2), left (3), pick-up (4), and drop-off (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. States\n",
    "\n",
    "Resets the state of the environment and returns an initial observation (state).\n",
    "\n",
    "The current state is from :\n",
    "* current taxi row position\n",
    "* current taxi colum position\n",
    "* passenger location (Blue or in taxi) from 0: R, 1: G, 2: Y, 3: B; 4: in taxi.\n",
    "* destination location (Magenta) from 0: R, 1: G, 2: Y, 3: B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 2206\n",
      "12\n",
      "2\n",
      "1\n",
      "2\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O|\u001b[43m \u001b[0m|O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "print \"Current state: \" + str(env.s)\n",
    "for p in env.decode(env.s): print p\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat previous cell for a few times.\n",
    "\n",
    "In taxi problem, the colors mean:\n",
    "* blue: passenger's current position\n",
    "* magenta: destination\n",
    "* yellow: empty taxi\n",
    "* green: full taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Actions\n",
    "\n",
    "Remember that the taxi agent can perform the following actions:\n",
    "* 0: \"South\", \n",
    "* 1: \"North\", \n",
    "* 2: \"East\", \n",
    "* 3: \"West\", \n",
    "* 4: \"Pickup\", \n",
    "* 5: \"Dropoff\"\n",
    "\n",
    "Let's set the state to 124.\n",
    "Let the taxi agent perform some actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.s = 124\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `step()`\n",
    "\n",
    "Run one timestep of the environment's dynamics. \n",
    "It returns a tuple (observation, reward, done, info)\n",
    "* observation (object): agent's observation of the current environment\n",
    "* reward (float) : amount of reward returned after previous action\n",
    "* done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "* info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "\n",
    "Essentially the empty taxi is supposed to: \n",
    "* move toward the blue letter, \n",
    "* pickup the passenger (now the taxi is green), \n",
    "* drive to the magenta letter, and \n",
    "* drop the passenger (the taxi is yellow again).\n",
    "\n",
    "It is obvious that we should start with moving \"East\" env.step(2). Index 2 is for moving \"East\"\n",
    "We will do the followings:\n",
    "* Perform \"Pickup\" step(4) (although the passenger is not here)\n",
    "* Perform \"East\" step(2)\n",
    "* Perform \"Pickup\" step(4)\n",
    "* Perform \"West\" step(3)\n",
    "* Perform \"South\" step(0) for 5 times\n",
    "* Perfomr \"Dropoff\" (5)\n",
    "* Perform \"West\" step(3) for 4 times\n",
    "* Perfomr \"Dropoff\" (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "reward: -10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (East)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(2)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[42mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[42m_\u001b[0m:G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(3)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    env.step(0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n",
      "reward: -10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(5)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n",
      "reward: 20\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(5)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)\n",
    "print \"done: \" + str(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "You have probably figured out the rewards:\n",
    "* Perform any movements: -1\n",
    "* Pick up or drop off at the wrong position: -10\n",
    "* Drop off the passenger at the right position: 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ramdon Agent: \n",
    "\n",
    "We will use the funciton env.action_space.sample(); you could run the following cell for a few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good does behaving completely random do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 4021 Steps with a total reward of -15970\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "counter = 0\n",
    "g = 0\n",
    "reward = None\n",
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "    g += reward\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter,g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may luck out and solve the environment fairly quickly, but on average, a completely random policy will solve this environment in about ???? steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent with Basic Reinforcement Learning: Q-Learning\n",
    "\n",
    "In order to maximize our reward, we will have to have the algorithm remember its actions and their associated rewards. Here, the algorithm’s memory is going to be a Q action value table.\n",
    "\n",
    "To manage this Q table, we will use a NumPy array. The size of this table will be the number of states (2520) by the number of possible actions (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| :\u001b[43m \u001b[0m:\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "\n",
      "Initial State = 924\n",
      "Step: 0, Action: 0, Reward: -1, Q[924] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 1, Action: 0, Reward: -1, Q[1104] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 2, Action: 0, Reward: -1, Q[1284] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 3, Action: 0, Reward: -1, Q[1464] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 4, Action: 0, Reward: -1, Q[1644] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 5, Action: 0, Reward: -1, Q[1824] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 6, Action: 0, Reward: -1, Q[2004] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 7, Action: 0, Reward: -1, Q[2184] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 8, Action: 0, Reward: -1, Q[2364] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 9, Action: 1, Reward: -1, Q[2364] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 10, Action: 1, Reward: -1, Q[2184] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 11, Action: 1, Reward: -1, Q[2004] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 12, Action: 1, Reward: -1, Q[1824] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 13, Action: 1, Reward: -1, Q[1644] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 14, Action: 1, Reward: -1, Q[1464] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 15, Action: 1, Reward: -1, Q[1284] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 16, Action: 1, Reward: -1, Q[1104] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 17, Action: 1, Reward: -1, Q[924] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n",
      "Step: 18, Action: 0, Reward: -1, Q[744] \t[-0.618  0.     0.     0.     0.     0.   ]\n",
      "Step: 19, Action: 2, Reward: -1, Q[924] \t[-0.618 -0.618 -0.618  0.     0.     0.   ]\n",
      "Final State = 956\n",
      "Solved in 5006 Steps with a total reward of -11087\n",
      "[-0.618 -0.618 -0.618 -0.618 -6.18  12.36 ]\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = np.zeros([n_states, n_actions])\n",
    "\n",
    "episodes = 1\n",
    "G = 0\n",
    "counter = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    firstState = state\n",
    "    print(\"Initial State = {}\".format(state))\n",
    "    while reward != 20:\n",
    "        action = np.argmax(Q[state])  #1\n",
    "        state2, reward, done, info = env.step(action) #2\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) #3\n",
    "        \n",
    "        if counter < 20:\n",
    "            print(\"Step: {}, Action: {}, Reward: {}, Q[{}] \\t{}\".format(counter, action, reward, state, Q[state]))\n",
    "\n",
    "        counter += 1\n",
    "        G += reward\n",
    "        state = state2\n",
    "        \n",
    "finalState = state\n",
    "print(\"Final State = {}\".format(finalState))\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter, G))\n",
    "\n",
    "print Q[finalState]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (#1): The agent starts by choosing an action with the highest Q value for the current state using argmax. Argmax will return the index/action with the highest value for that state. Initially, our Q table will be all zeros. But, after every step, the Q values for state-action pairs will be updated.\n",
    "\n",
    "Second (#2): The agent then takes action and we store the future state as state2 (S t+1). This will allow the agent to compare the previous state to the new state.\n",
    "\n",
    "Third (#3): We update the state-action pair (St , At) for Q using the reward, and the max Q value for state2 (S t+1). This update is done using the action value formula (based upon the Bellman equation) and allows state-action pairs to be updated in a recursive fashion (based on future values). See the following Figure for the value iteration update.\n",
    "\n",
    "<img src=\"images/qlearn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run over multiple episodes so that we can converge on a optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Total Reward: -596\n",
      "Episode 200 Total Reward: -315\n",
      "Episode 300 Total Reward: -193\n",
      "Episode 400 Total Reward: 0\n",
      "Episode 500 Total Reward: -144\n",
      "Episode 600 Total Reward: -40\n",
      "Episode 700 Total Reward: -67\n",
      "Episode 800 Total Reward: -31\n",
      "Episode 900 Total Reward: -3\n",
      "Episode 1000 Total Reward: -21\n",
      "Episode 1100 Total Reward: -612\n",
      "Episode 1200 Total Reward: -114\n",
      "Episode 1300 Total Reward: -72\n",
      "Episode 1400 Total Reward: 5\n",
      "Episode 1500 Total Reward: 2\n",
      "Episode 1600 Total Reward: 4\n",
      "Episode 1700 Total Reward: 6\n",
      "Episode 1800 Total Reward: 2\n",
      "Episode 1900 Total Reward: 9\n",
      "Episode 2000 Total Reward: 4\n",
      "Episode 2100 Total Reward: 2\n",
      "Episode 2200 Total Reward: -1\n",
      "Episode 2300 Total Reward: 4\n",
      "Episode 2400 Total Reward: 3\n",
      "Episode 2500 Total Reward: 2\n",
      "Episode 2600 Total Reward: 4\n",
      "Episode 2700 Total Reward: 4\n",
      "Episode 2800 Total Reward: -10\n",
      "Episode 2900 Total Reward: 2\n",
      "Episode 3000 Total Reward: 1\n",
      "Episode 3100 Total Reward: -2\n",
      "Episode 3200 Total Reward: 7\n",
      "Episode 3300 Total Reward: 0\n",
      "Episode 3400 Total Reward: 5\n",
      "Episode 3500 Total Reward: -4\n",
      "Episode 3600 Total Reward: -14\n",
      "Episode 3700 Total Reward: -6\n",
      "Episode 3800 Total Reward: 2\n",
      "Episode 3900 Total Reward: 3\n",
      "Episode 4000 Total Reward: 11\n",
      "Episode 4100 Total Reward: 0\n",
      "Episode 4200 Total Reward: 4\n",
      "Episode 4300 Total Reward: 0\n",
      "Episode 4400 Total Reward: 2\n",
      "Episode 4500 Total Reward: 1\n",
      "Episode 4600 Total Reward: 2\n",
      "Episode 4700 Total Reward: 6\n",
      "Episode 4800 Total Reward: 0\n",
      "Episode 4900 Total Reward: 2\n",
      "Episode 5000 Total Reward: -1\n",
      "Episode 5100 Total Reward: 10\n",
      "Episode 5200 Total Reward: 4\n",
      "Episode 5300 Total Reward: 8\n",
      "Episode 5400 Total Reward: 8\n",
      "Episode 5500 Total Reward: -16\n",
      "Episode 5600 Total Reward: 8\n",
      "Episode 5700 Total Reward: -2\n",
      "Episode 5800 Total Reward: 7\n",
      "Episode 5900 Total Reward: -7\n",
      "Episode 6000 Total Reward: -6\n",
      "Episode 6100 Total Reward: 4\n",
      "Episode 6200 Total Reward: -5\n",
      "Episode 6300 Total Reward: -4\n",
      "Episode 6400 Total Reward: 9\n",
      "Episode 6500 Total Reward: -11\n",
      "Episode 6600 Total Reward: 3\n",
      "Episode 6700 Total Reward: -5\n",
      "Episode 6800 Total Reward: 3\n",
      "Episode 6900 Total Reward: -10\n",
      "Episode 7000 Total Reward: -4\n",
      "Episode 7100 Total Reward: -6\n",
      "Episode 7200 Total Reward: 4\n",
      "Episode 7300 Total Reward: 5\n",
      "Episode 7400 Total Reward: -3\n",
      "Episode 7500 Total Reward: -11\n",
      "Episode 7600 Total Reward: 3\n",
      "Episode 7700 Total Reward: 9\n",
      "Episode 7800 Total Reward: 2\n",
      "Episode 7900 Total Reward: 8\n",
      "Episode 8000 Total Reward: -2\n",
      "Episode 8100 Total Reward: 4\n",
      "Episode 8200 Total Reward: -2\n",
      "Episode 8300 Total Reward: -6\n",
      "Episode 8400 Total Reward: 8\n",
      "Episode 8500 Total Reward: 3\n",
      "Episode 8600 Total Reward: 0\n",
      "Episode 8700 Total Reward: -9\n",
      "Episode 8800 Total Reward: 5\n",
      "Episode 8900 Total Reward: 1\n",
      "Episode 9000 Total Reward: 6\n",
      "Episode 9100 Total Reward: 9\n",
      "Episode 9200 Total Reward: -9\n",
      "Episode 9300 Total Reward: 7\n",
      "Episode 9400 Total Reward: -6\n",
      "Episode 9500 Total Reward: 1\n",
      "Episode 9600 Total Reward: -10\n",
      "Episode 9700 Total Reward: 10\n",
      "Episode 9800 Total Reward: 4\n",
      "Episode 9900 Total Reward: -1\n",
      "Episode 10000 Total Reward: 3\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "rewardTracker = []\n",
    "\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state]) \n",
    "        state2, reward, done, info = env.step(action) \n",
    "        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,G))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have learned the optimal Q Values we have developed a optimal policy and have no need to train the agent anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :\u001b[43mO\u001b[0m: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |\u001b[43mO\u001b[0m| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : :\u001b[43m \u001b[0m: |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |\u001b[43mO\u001b[0m| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |\u001b[43mO\u001b[0m| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (North)\n",
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[42mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |\u001b[42mO\u001b[0m| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |\u001b[42mO\u001b[0m| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : :\u001b[42m_\u001b[0m: |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : :\u001b[42m_\u001b[0m: : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O|\u001b[42m_\u001b[0m|O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : :\u001b[42m_\u001b[0m: :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: :\u001b[42m_\u001b[0m: : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R:\u001b[42m_\u001b[0m: : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|\u001b[42mO\u001b[0m| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| :\u001b[42m_\u001b[0m: |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |\u001b[42mO\u001b[0m|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : :\u001b[42m_\u001b[0m: : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |\u001b[42mO\u001b[0m| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : :\u001b[42m_\u001b[0m: : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |\u001b[42mO\u001b[0m| |O|O|O| |\n",
      "| : : :\u001b[35mY\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35m\u001b[42mY\u001b[0m\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :R: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :\u001b[35m\u001b[42mY\u001b[0m\u001b[0m: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "done = None\n",
    "\n",
    "while done != True:\n",
    "    # We simply take the action with the highest Q Value\n",
    "    action = np.argmax(Q[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
