{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f71058e35f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above output **False** means you can't use cuda acceleration. Don't worry It'll still work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN\n",
    "\n",
    "Network structure:\n",
    "\n",
    "Conv2d -> Activation -> MaxPooling -> Conv2d -> Activation -> MaxPooling -> Flattening -> FullyConnected\n",
    "\n",
    "Used function:\n",
    "- Convolution: nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "- Activation: nn.ReLU()\n",
    "- MaxPooling: nn.MaxPool2d(kernel_size)\n",
    "- FullyConnected: nn.Linear(input_channels, output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(1, 6, 5, padding=2),  # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(6, 16, 5),            # output shape (32, 14, 14)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.lin1 = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),   # fully connected layer, output 10 classes\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lin2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(84, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lin1): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (lin2): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=84, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if MNIST is already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_FM = False\n",
    "# Mnist digits dataset\n",
    "if not(os.path.exists('./dataset/FashionMNIST/')) or not os.listdir('./dataset/FashionMNIST/'):\n",
    "    # not mnist dir or mnist is empyt dir\n",
    "    DOWNLOAD_FM = True\n",
    "    \n",
    "print(DOWNLOAD_FM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: normalize image data so that grayscale data is between -1 and +1\n",
    "tf = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    root='./dataset/',\n",
    "    train=True,\n",
    "    transform=tf,\n",
    "    download=DOWNLOAD_FM,\n",
    ")\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick 2000 samples to speed up testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root='./dataset/',\n",
    "    train=False,\n",
    "    transform=tf\n",
    ")\n",
    "test_loader = Data.DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PIC_IDX = 4 # change this to see different picture\n",
    "print(train_data.train_data.size())                 # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())               # (60000)\n",
    "plt.imshow(train_data.train_data[PIC_IDX].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[PIC_IDX])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we are ready to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5               # train the training data n times, to save time, we just train 1 epoch\n",
    "LR = 0.01              # learning rate\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/5|  step: 0/938| train loss: 0.362|  Validation loss: 0.387|  accuracy: 0.870\n",
      "epoch: 0/5|  step: 100/938| train loss: 0.318|  Validation loss: 0.381|  accuracy: 0.868\n",
      "epoch: 0/5|  step: 200/938| train loss: 0.276|  Validation loss: 0.392|  accuracy: 0.860\n",
      "epoch: 0/5|  step: 300/938| train loss: 0.355|  Validation loss: 0.377|  accuracy: 0.871\n",
      "epoch: 0/5|  step: 400/938| train loss: 0.570|  Validation loss: 0.370|  accuracy: 0.874\n",
      "epoch: 0/5|  step: 500/938| train loss: 0.446|  Validation loss: 0.400|  accuracy: 0.864\n",
      "epoch: 0/5|  step: 600/938| train loss: 0.326|  Validation loss: 0.363|  accuracy: 0.875\n",
      "epoch: 0/5|  step: 700/938| train loss: 0.323|  Validation loss: 0.370|  accuracy: 0.868\n",
      "epoch: 0/5|  step: 800/938| train loss: 0.362|  Validation loss: 0.393|  accuracy: 0.860\n",
      "epoch: 0/5|  step: 900/938| train loss: 0.256|  Validation loss: 0.384|  accuracy: 0.869\n",
      "epoch: 1/5|  step: 0/938| train loss: 0.200|  Validation loss: 0.406|  accuracy: 0.869\n",
      "epoch: 1/5|  step: 100/938| train loss: 0.391|  Validation loss: 0.376|  accuracy: 0.876\n",
      "epoch: 1/5|  step: 200/938| train loss: 0.249|  Validation loss: 0.376|  accuracy: 0.863\n",
      "epoch: 1/5|  step: 300/938| train loss: 0.273|  Validation loss: 0.375|  accuracy: 0.871\n",
      "epoch: 1/5|  step: 400/938| train loss: 0.445|  Validation loss: 0.387|  accuracy: 0.869\n",
      "epoch: 1/5|  step: 500/938| train loss: 0.260|  Validation loss: 0.452|  accuracy: 0.858\n",
      "epoch: 1/5|  step: 600/938| train loss: 0.322|  Validation loss: 0.376|  accuracy: 0.875\n",
      "epoch: 1/5|  step: 700/938| train loss: 0.306|  Validation loss: 0.379|  accuracy: 0.869\n",
      "epoch: 1/5|  step: 800/938| train loss: 0.296|  Validation loss: 0.380|  accuracy: 0.871\n",
      "epoch: 1/5|  step: 900/938| train loss: 0.187|  Validation loss: 0.406|  accuracy: 0.873\n",
      "epoch: 2/5|  step: 0/938| train loss: 0.313|  Validation loss: 0.367|  accuracy: 0.875\n",
      "epoch: 2/5|  step: 100/938| train loss: 0.401|  Validation loss: 0.437|  accuracy: 0.845\n",
      "epoch: 2/5|  step: 200/938| train loss: 0.238|  Validation loss: 0.374|  accuracy: 0.873\n",
      "epoch: 2/5|  step: 300/938| train loss: 0.421|  Validation loss: 0.377|  accuracy: 0.877\n",
      "epoch: 2/5|  step: 400/938| train loss: 0.304|  Validation loss: 0.376|  accuracy: 0.875\n",
      "epoch: 2/5|  step: 500/938| train loss: 0.298|  Validation loss: 0.381|  accuracy: 0.869\n",
      "epoch: 2/5|  step: 600/938| train loss: 0.289|  Validation loss: 0.367|  accuracy: 0.877\n",
      "epoch: 2/5|  step: 700/938| train loss: 0.306|  Validation loss: 0.362|  accuracy: 0.878\n",
      "epoch: 2/5|  step: 800/938| train loss: 0.557|  Validation loss: 0.420|  accuracy: 0.862\n",
      "epoch: 2/5|  step: 900/938| train loss: 0.373|  Validation loss: 0.370|  accuracy: 0.877\n",
      "epoch: 3/5|  step: 0/938| train loss: 0.247|  Validation loss: 0.380|  accuracy: 0.871\n",
      "epoch: 3/5|  step: 100/938| train loss: 0.406|  Validation loss: 0.395|  accuracy: 0.875\n",
      "epoch: 3/5|  step: 200/938| train loss: 0.255|  Validation loss: 0.361|  accuracy: 0.878\n",
      "epoch: 3/5|  step: 300/938| train loss: 0.402|  Validation loss: 0.403|  accuracy: 0.867\n",
      "epoch: 3/5|  step: 400/938| train loss: 0.218|  Validation loss: 0.401|  accuracy: 0.864\n",
      "epoch: 3/5|  step: 500/938| train loss: 0.259|  Validation loss: 0.374|  accuracy: 0.880\n",
      "epoch: 3/5|  step: 600/938| train loss: 0.300|  Validation loss: 0.402|  accuracy: 0.864\n",
      "epoch: 3/5|  step: 700/938| train loss: 0.527|  Validation loss: 0.395|  accuracy: 0.873\n",
      "epoch: 3/5|  step: 800/938| train loss: 0.411|  Validation loss: 0.400|  accuracy: 0.858\n",
      "epoch: 3/5|  step: 900/938| train loss: 0.217|  Validation loss: 0.370|  accuracy: 0.874\n",
      "epoch: 4/5|  step: 0/938| train loss: 0.332|  Validation loss: 0.391|  accuracy: 0.868\n",
      "epoch: 4/5|  step: 100/938| train loss: 0.190|  Validation loss: 0.386|  accuracy: 0.871\n",
      "epoch: 4/5|  step: 200/938| train loss: 0.412|  Validation loss: 0.365|  accuracy: 0.874\n",
      "epoch: 4/5|  step: 300/938| train loss: 0.375|  Validation loss: 0.397|  accuracy: 0.864\n",
      "epoch: 4/5|  step: 400/938| train loss: 0.234|  Validation loss: 0.387|  accuracy: 0.872\n",
      "epoch: 4/5|  step: 500/938| train loss: 0.331|  Validation loss: 0.390|  accuracy: 0.871\n",
      "epoch: 4/5|  step: 600/938| train loss: 0.180|  Validation loss: 0.400|  accuracy: 0.873\n",
      "epoch: 4/5|  step: 700/938| train loss: 0.310|  Validation loss: 0.360|  accuracy: 0.881\n",
      "epoch: 4/5|  step: 800/938| train loss: 0.361|  Validation loss: 0.435|  accuracy: 0.867\n",
      "epoch: 4/5|  step: 900/938| train loss: 0.578|  Validation loss: 0.360|  accuracy: 0.878\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = [], []\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    running_loss = 0\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps, hidden = cnn(images)\n",
    "        loss = loss_func(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "    \n",
    "        if step%100 == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                cnn.eval()\n",
    "                for images, labels in test_loader:\n",
    "    \n",
    "                    log_ps, hidden = cnn(images)\n",
    "                    test_loss += loss_func(log_ps, labels)\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "                print(\"epoch: {}/{}| \".format(epoch+1, EPOCH),\n",
    "                      \"step: {}/{}|\".format(step,len(train_loader)),\n",
    "                      \"train loss: {:.3f}| \".format(loss.data.numpy()),\n",
    "                      \"Validation loss: {:.3f}| \".format(test_loss/len(test_loader)),\n",
    "                      \"accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "                cnn.train()\n",
    "            \n",
    "#                       \"training loss: {:.3f}| \".format(running_loss/len(train_loader)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print 10 predictions from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output, _ = cnn(test_x[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10].numpy(), 'real number')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
